from dataclasses import dataclass
from typing import Optional, Tuple, List, Dict, Any

import math
import torch
import torch.nn as nn
import torch.nn.functional as F

# ------------------------------
# Config
# ------------------------------

@dataclass
class GPTConfig:
    # Model dims
    n_vocab: int = 50257
    n_ctx:   int = 8192
    n_embd:  int = 768
    n_head:  int = 12
    n_layer: int = 12

    # Regularization / numerics
    pdrop_attn: float = 0.1
    pdrop_resid: float = 0.1
    pdrop_emb:  float = 0.1
    layer_norm_epsilon: float = 1e-5

    # Attention
    use_mqa: bool = True        # Multi‑Query Attention (shared K/V)
    mqa_groups: int = 1         # 1 => classic MQA

    # MoE MLP
    moe_enabled: bool = True
    moe_experts: int = 16
    moe_top_k: int = 2          # {1,2}
    mlp_ratio: int = 4          # hidden = mlp_ratio * n_embd
    moe_dropout: float = 0.1
    router_bias: bool = True
    expert_capacity: int = 0    # 0 => auto (ceil(tokens * top_k / experts))
    router_jitter_noise: float = 0.0
    router_ignore_padding_tokens: bool = True
    pad_token_id: int = 0

# ------------------------------
# Helper ops
# ------------------------------

def gelu(x: torch.Tensor) -> torch.Tensor:
    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))

class LayerNorm(nn.Module):
    def __init__(self, n_state: int, eps: float = 1e-5):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(n_state))
        self.bias = nn.Parameter(torch.zeros(n_state))
        self.eps = eps
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return F.layer_norm(x, x.shape[-1:], self.weight, self.bias, self.eps)

# ------------------------------
# Attention (Dense causal + optional MQA)
# ------------------------------

class CausalSelfAttention(nn.Module):
    def __init__(self, cfg: GPTConfig):
        super().__init__()
        assert cfg.n_embd % cfg.n_head == 0
        self.cfg = cfg
        self.n_head = cfg.n_head
        self.head_dim = cfg.n_embd // cfg.n_head

        if cfg.use_mqa:
            self.q_proj = nn.Linear(cfg.n_embd, cfg.n_embd)
            self.k_proj = nn.Linear(cfg.n_embd, self.head_dim * cfg.mqa_groups)
            self.v_proj = nn.Linear(cfg.n_embd, self.head_dim * cfg.mqa_groups)
        else:
            self.qkv = nn.Linear(cfg.n_embd, 3 * cfg.n_embd)

        self.out_proj = nn.Linear(cfg.n_embd, cfg.n_embd)
        self.attn_drop = nn.Dropout(cfg.pdrop_attn)
        self.resid_drop = nn.Dropout(cfg.pdrop_resid)
        self.apply(self._init_weights)

    def _init_weights(self, m: nn.Module):
        if isinstance(m, nn.Linear):
            nn.init.normal_((m.weight), mean=0.0, std=0.02)
            if m.bias is not None:
                nn.init.zeros_(m.bias)

    def forward(self,
                x: torch.Tensor,
                past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
                use_cache: bool = True) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:
        B, T, C = x.shape
        if self.cfg.use_mqa:
            q = self.q_proj(x)
            k = self.k_proj(x)
            v = self.v_proj(x)
            q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)
            G = self.cfg.mqa_groups
            k = k.view(B, T, G, self.head_dim).transpose(1, 2)
            v = v.view(B, T, G, self.head_dim).transpose(1, 2)
            if G == 1:
                k = k.repeat(1, self.n_head, 1, 1)
                v = v.repeat(1, self.n_head, 1, 1)
            else:
                assert self.n_head % G == 0
                rep = self.n_head // G
                k = k.repeat_interleave(rep, dim=1)
                v = v.repeat_interleave(rep, dim=1)
        else:
            qkv = self.qkv(x)
            q, k, v = qkv.split(C, dim=2)
            q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)
            k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)
            v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)

        if past_kv is not None:
            pk, pv = past_kv
            k = torch.cat([pk, k], dim=2)
            v = torch.cat([pv, v], dim=2)
        present = (k, v) if use_cache else None

        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        T_k = k.size(2)
        mask = torch.tril(torch.ones(T, T_k, device=x.device, dtype=torch.bool))
        att = att.masked_fill(~mask.view(1, 1, T, T_k), float("-inf"))
        att = F.softmax(att, dim=-1)
        att = self.attn_drop(att)
        y = att @ v
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        y = self.out_proj(y)
        y = self.resid_drop(y)
        return y, present

# ------------------------------
# MoE: Experts, Router (capacity + jitter + padding‑aware), MLP
# ------------------------------

class Expert(nn.Module):
    def __init__(self, dim_in: int, dim_hidden: int, dim_out: int, pdrop: float):
        super().__init__()
        self.wi = nn.Linear(dim_in, dim_hidden, bias=False)
        self.wo = nn.Linear(dim_hidden, dim_out, bias=False)
        self.drop = nn.Dropout(pdrop)
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.drop(self.wo(gelu(self.wi(x))))

class Router(nn.Module):
    def __init__(self, num_experts: int, hidden_size: int, bias: bool, jitter_std: float = 0.0):
        super().__init__()
        self.classifier = nn.Linear(hidden_size, num_experts, bias=bias)
        self.jitter_std = jitter_std
        nn.init.normal_(self.classifier.weight, mean=0.0, std=0.02)
        if self.classifier.bias is not None:
            nn.init.zeros_(self.classifier.bias)
    def forward(self, x: torch.Tensor, padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        logits = self.classifier(x)
        if self.jitter_std > 0 and self.training:
            logits = logits + torch.randn_like(logits) * self.jitter_std
        if padding_mask is not None:
            logits = logits.masked_fill(padding_mask.unsqueeze(-1), float("-inf"))
        probs = F.softmax(logits, dim=-1)
        return probs, logits

class MoEMLP(nn.Module):
    """Top‑K router with per‑expert capacity & token dropping.
       Returns (y, aux_losses_dict, diagnostics_dict).
    """
    def __init__(self, cfg: GPTConfig):
        super().__init__()
        self.cfg = cfg
        inner = cfg.mlp_ratio * cfg.n_embd
        E = cfg.moe_experts
        self.experts = nn.ModuleList([Expert(cfg.n_embd, inner, cfg.n_embd, cfg.moe_dropout) for _ in range(E)])
        self.router = Router(E, cfg.n_embd, cfg.router_bias, cfg.router_jitter_noise)
        self.resid_drop = nn.Dropout(cfg.pdrop_resid)

    @staticmethod
    def _capacity(tokens: int, top_k: int, E: int, cap: int) -> int:
        return cap if cap > 0 else math.ceil(tokens * top_k / E)

    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], Dict[str, Any]]:
        B, T, C = x.shape
        E = self.cfg.moe_experts
        K = self.cfg.moe_top_k

        pad_mask = None
        if self.cfg.router_ignore_padding_tokens and attention_mask is not None:
            pad_mask = ~attention_mask.bool()
        probs, logits = self.router(x, padding_mask=pad_mask)

        topk_vals, topk_idx = torch.topk(probs, k=K, dim=-1)
        topk_vals = topk_vals / (topk_vals.sum(dim=-1, keepdim=True) + 1e-9)

        N = B * T
        flat_idx = topk_idx.view(N, K)
        flat_wts = topk_vals.view(N, K)

        capacity = self._capacity(N, K, E, self.cfg.expert_capacity)
        y = torch.zeros_like(x).view(N, C)
        dropped = torch.zeros(E, device=x.device, dtype=torch.long)

        importance = probs.view(N, E).sum(dim=0)
        load = torch.zeros(E, device=x.device)

        for e in range(E):
            mask_e = (flat_idx == e)
            if not mask_e.any():
                continue
            sel_tokens = torch.nonzero(mask_e.any(dim=1), as_tuple=False).squeeze(-1)
            weights_e = torch.where(mask_e, flat_wts, torch.zeros_like(flat_wts)).max(dim=1).values
            weights_e = weights_e.index_select(0, sel_tokens)
            _, order = torch.sort(weights_e, descending=True)
            sel_tokens = sel_tokens.index_select(0, order)
            take = sel_tokens[:capacity]
            drop = sel_tokens[capacity:]
            dropped[e] = drop.numel()
            load[e] = take.numel()
            if take.numel() > 0:
                x_take = x.view(N, C).index_select(0, take)
                out_take = self.experts[e](x_take)
                w_e_full = weights_e.index_select(0, order)[:take.numel()].unsqueeze(-1)
                y.index_copy_(0, take, w_e_full * out_take)

        y = y.view(B, T, C)
        y = self.resid_drop(y)

        importance = importance / (importance.sum() + 1e-9)
        load = load / (load.sum() + 1e-9)
        load_balance_loss = (E * (importance * load).sum())
        mean_gate = probs.mean(dim=(0,1)) + 1e-9
        entropy_loss = -torch.sum(mean_gate * torch.log(mean_gate)) / math.log(E)

        aux = {
            "load_balance": load_balance_loss,
            "entropy": entropy_loss,
            "dropped_frac": dropped.sum() / max(N * K, 1)
        }
        diag: Dict[str, Any] = {
            "router_logits": logits.detach(),
            "topk_index": topk_idx.detach(),
            "topk_vals": topk_vals.detach(),
            "dropped_per_expert": dropped.detach(),
            "capacity": capacity
        }
        return y, aux, diag

class DenseMLP(nn.Module):
    def __init__(self, cfg: GPTConfig):
        super().__init__()
        inner = cfg.mlp_ratio * cfg.n_embd
        self.fc = nn.Linear(cfg.n_embd, inner)
        self.proj = nn.Linear(inner, cfg.n_embd)
        self.drop = nn.Dropout(cfg.pdrop_resid)
        nn.init.normal_(self.fc.weight, mean=0.0, std=0.02)
        nn.init.zeros_(self.fc.bias)
        nn.init.normal_(self.proj.weight, mean=0.0, std=0.02)
        nn.init.zeros_(self.proj.bias)
    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):
        y = self.drop(self.proj(gelu(self.fc(x))))
        aux = {"load_balance": x.new_zeros(()), "entropy": x.new_zeros(()), "dropped_frac": x.new_zeros(())}
        diag: Dict[str, Any] = {}
        return y, aux, diag

# ------------------------------
# Transformer block (Pre‑LN)
# ------------------------------

class Block(nn.Module):
    def __init__(self, cfg: GPTConfig, n_layers: int):
        super().__init__()
        self.ln_1 = LayerNorm(cfg.n_embd, cfg.layer_norm_epsilon)
        self.attn = CausalSelfAttention(cfg)
        self.ln_2 = LayerNorm(cfg.n_embd, cfg.layer_norm_epsilon)
        self.mlp = MoEMLP(cfg) if cfg.moe_enabled else DenseMLP(cfg)
        scale = 1.0 / math.sqrt(n_layers)
        with torch.no_grad():
            self.attn.out_proj.weight.mul_(scale)
            if isinstance(self.mlp, MoEMLP):
                for e in self.mlp.experts:
                    e.wo.weight.mul_(scale)
            else:
                self.mlp.proj.weight.mul_(scale)

    def forward(self,
                x: torch.Tensor,
                attention_mask: Optional[torch.Tensor] = None,
                past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
                use_cache: bool = True):
        a, present = self.attn(self.ln_1(x), past_kv=past_kv, use_cache=use_cache)
        x = x + a
        m, aux, diag = self.mlp(self.ln_2(x), attention_mask=attention_mask)
        x = x + m
        return x, present, aux, diag

# ------------------------------
# GPT model (text)
# ------------------------------

class GPT(nn.Module):
    def __init__(self, cfg: GPTConfig):
        super().__init__()
        self.cfg = cfg
        self.wte = nn.Embedding(cfg.n_vocab, cfg.n_embd)
        self.wpe = nn.Embedding(cfg.n_ctx, cfg.n_embd)
        self.emb_drop = nn.Dropout(cfg.pdrop_emb)
        self.h = nn.ModuleList([Block(cfg, cfg.n_layer) for _ in range(cfg.n_layer)])
        self.ln_f = LayerNorm(cfg.n_embd, cfg.layer_norm_epsilon)
        self.lm_head = nn.Linear(cfg.n_embd, cfg.n_vocab, bias=False)
        self.lm_head.weight = self.wte.weight
        self.apply(self._init_weights)

    def _init_weights(self, m: nn.Module):
        if isinstance(m, nn.Embedding):
            nn.init.normal_((m.weight), mean=0.0, std=0.02)
        elif isinstance(m, nn.Linear):
            nn.init.normal_(m.weight, mean=0.0, std=0.02)
            if m.bias is not None:
                nn.init.zeros_(m.bias)

    def forward(self,
                idx: torch.Tensor,
                attention_mask: Optional[torch.Tensor] = None,
                past: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
                use_cache: bool = True,
                labels: Optional[torch.Tensor] = None):
        B, T = idx.shape
        if T > self.cfg.n_ctx:
            raise ValueError("Sequence length exceeds model context")
        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)
        x = self.wte(idx) + self.wpe(pos)[None, :, :]
        x = self.emb_drop(x)

        if attention_mask is None:
            attention_mask = torch.ones((B, T), dtype=torch.bool, device=idx.device)
        elif attention_mask.dtype != torch.bool:
            attention_mask = attention_mask.to(torch.bool)

        presents: List[Tuple[torch.Tensor, torch.Tensor]] = []
        aux_sums = {"load_balance": 0.0, "entropy": 0.0, "dropped_frac": 0.0}
        if past is None:
            past = [None] * len(self.h)
        for i, block in enumerate(self.h):
            x, present, aux, _ = block(x, attention_mask=attention_mask, past_kv=past[i], use_cache=use_cache)
            if use_cache:
                presents.append(present)
            for k, v in aux.items():
                aux_sums[k] = (aux_sums[k] + v) if isinstance(aux_sums[k], torch.Tensor) else v
        x = self.ln_f(x)
        logits = self.lm_head(x)

        lm_loss = None
        if labels is not None:
            lm_loss = F.cross_entropy(logits[:, :-1, :].contiguous().view(-1, logits.size(-1)),
                                      labels[:, 1:].contiguous().view(-1),
                                      ignore_index=-100)
        aux_terms = {k: (v / len(self.h)) if isinstance(v, torch.Tensor) else torch.tensor(v, device=logits.device) for k, v in aux_sums.items()}
        return logits, (presents if use_cache else None), lm_loss, aux_terms

# ------------------------------
# Vision encoder & Cross‑Attention (Flamingo‑style)
# ------------------------------

class VisionEncoderViT(nn.Module):
    def __init__(self, img_size: int = 224, patch: int = 16, width: int = 768, layers: int = 12, heads: int = 12):
        super().__init__()
        assert img_size % patch == 0
        self.grid = img_size // patch
        self.embed = nn.Conv2d(3, width, kernel_size=patch, stride=patch)
        self.pos = nn.Parameter(torch.zeros(1, self.grid * self.grid, width))
        enc = nn.TransformerEncoderLayer(d_model=width, nhead=heads, dim_feedforward=4*width, activation="gelu", batch_first=True)
        self.encoder = nn.TransformerEncoder(enc, num_layers=layers)
        nn.init.normal_((self.pos), mean=0.0, std=0.02)
    def forward(self, images: torch.Tensor) -> torch.Tensor:
        x = self.embed(images)                # (B, C, H/ps, W/ps)
        x = x.flatten(2).transpose(1, 2)      # (B, N, C)
        x = x + self.pos[:, :x.size(1), :]
        x = self.encoder(x)
        return x

class CrossAttention(nn.Module):
    def __init__(self, n_embd: int, n_head: int, pdrop_attn: float, pdrop_resid: float):
        super().__init__()
        assert n_embd % n_head == 0
        self.n_head = n_head
        self.head_dim = n_embd // n_head
        self.q_proj = nn.Linear(n_embd, n_embd)
        self.k_proj = nn.Linear(n_embd, n_embd)
        self.v_proj = nn.Linear(n_embd, n_embd)
        self.out_proj = nn.Linear(n_embd, n_embd)
        self.attn_drop = nn.Dropout(pdrop_attn)
        self.resid_drop = nn.Dropout(pdrop_resid)
        for m in [self.q_proj, self.k_proj, self.v_proj, self.out_proj]:
            nn.init.normal_(m.weight, mean=0.0, std=0.02)
            if m.bias is not None:
                nn.init.zeros_(m.bias)
    def forward(self, x_txt: torch.Tensor, x_vis: torch.Tensor) -> torch.Tensor:
        B, T, C = x_txt.shape
        Nv = x_vis.size(1)
        q = self.q_proj(x_txt).view(B, T, self.n_head, self.head_dim).transpose(1, 2)
        k = self.k_proj(x_vis).view(B, Nv, self.n_head, self.head_dim).transpose(1, 2)
        v = self.v_proj(x_vis).view(B, Nv, self.n_head, self.head_dim).transpose(1, 2)
        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        att = F.softmax(att, dim=-1)
        att = self.attn_drop(att)
        y = att @ v
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        y = self.out_proj(y)
        y = self.resid_drop(y)
        return y

class GPTVLM(GPT):
    def __init__(self, cfg: GPTConfig,
                 vision_img_size: int = 224,
                 vision_patch: int = 16,
                 vision_width: int = 768,
                 vision_layers: int = 12,
                 vision_heads: int = 12,
                 cross_attn_layers: Optional[List[int]] = None):
        super().__init__(cfg)
        self.vision = VisionEncoderViT(vision_img_size, vision_patch, vision_width, vision_layers, vision_heads)
        self.vision_proj = nn.Linear(vision_width, cfg.n_embd)
        nn.init.normal_(self.vision_proj.weight, mean=0.0, std=0.02)
        nn.init.zeros_(self.vision_proj.bias)
        if cross_attn_layers is None:
            cross_attn_layers = list(range(0, cfg.n_layer, 4))
        self.cross_attn_idx = set(cross_attn_layers)
        self.cross_blocks = nn.ModuleDict({str(i): CrossAttention(cfg.n_embd, cfg.n_head, cfg.pdrop_attn, cfg.pdrop_resid)
                                           for i in self.cross_attn_idx})
        scale = 1.0 / math.sqrt(cfg.n_layer)
        for i in self.cross_attn_idx:
            with torch.no_grad():
                self.cross_blocks[str(i)].out_proj.weight.mul_(scale)
    def forward(self,
                idx: torch.Tensor,
                attention_mask: Optional[torch.Tensor] = None,
                images: Optional[torch.Tensor] = None,
                past: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
                use_cache: bool = True,
                labels: Optional[torch.Tensor] = None):
        B, T = idx.shape
        if T > self.cfg.n_ctx:
            raise ValueError("Sequence length exceeds model context")
        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)
        x = self.wte(idx) + self.wpe(pos)[None, :, :]
        x = self.emb_drop(x)
        if attention_mask is None:
            attention_mask = torch.ones((B, T), dtype=torch.bool, device=idx.device)
        elif attention_mask.dtype != torch.bool:
            attention_mask = attention_mask.to(torch.bool)
        vis_mem = None
        if images is not None:
            v = self.vision(images)
            v = self.vision_proj(v)
            vis_mem = v
        presents: List[Tuple[torch.Tensor, torch.Tensor]] = []
        aux_sums = {"load_balance": 0.0, "entropy": 0.0, "dropped_frac": 0.0}
        if past is None:
            past = [None] * len(self.h)
        for i, block in enumerate(self.h):
            x, present, aux, _ = block(x, attention_mask=attention_mask, past_kv=past[i], use_cache=use_cache)
            if vis_mem is not None and i in self.cross_attn_idx:
                x = x + self.cross_blocks[str(i)](x, vis_mem)
            if use_cache:
                presents.append(present)
            for k, v in aux.items():
                aux_sums[k] = (aux_sums[k] + v) if isinstance(aux_sums[k], torch.Tensor) else v
        x = self.ln_f(x)
        logits = self.lm_head(x)
        lm_loss = None
        if labels is not None:
            lm_loss = F.cross_entropy(logits[:, :-1, :].contiguous().view(-1, logits.size(-1)),
                                      labels[:, 1:].contiguous().view(-1),
                                      ignore_index=-100)
        aux_terms = {k: (v / len(self.h)) if isinstance(v, torch.Tensor) else torch.tensor(v, device=logits.device) for k, v in aux_sums.items()}
        return logits, (presents if use_cache else None), lm_loss, aux_terms

# ------------------------------
# Speculative decoding (draft/oracle)
# ------------------------------

def run_speculative_decoding(oracle: GPT, draft: GPT,
                             input_ids: torch.Tensor,
                             max_new_tokens: int = 128,
                             draft_k: int = 4,
                             temperature: float = 1.0,
                             top_p: float = 1.0) -> torch.Tensor:
    device = next(oracle.parameters()).device
    x = input_ids.to(device)
    past_oracle = None
    past_draft = None
    for _ in range(max_new_tokens):
        draft_ids = []
        logits_d, past_draft, _, _ = draft(x[:, -oracle.cfg.n_ctx:], past=past_draft)
        next_d = logits_d[:, -1]
        probs = F.softmax(next_d / max(1e-5, temperature), dim=-1)
        if top_p < 1.0:
            sorted_probs, sorted_idx = torch.sort(probs, descending=True)
            cdf = torch.cumsum(sorted_probs, dim=-1)
            mask = cdf <= top_p
            mask[..., 0] = True
            probs = torch.where(mask, sorted_probs, torch.zeros_like(sorted_probs))
            probs = probs / probs.sum(dim=-1, keepdim=True)
            idx = torch.multinomial(probs, num_samples=1)
            token = sorted_idx.gather(-1, idx)
        else:
            token = torch.multinomial(probs, num_samples=1)
        draft_ids.append(token)
        tmp = token
        for _k in range(draft_k - 1):
            logits_d, past_draft, _, _ = draft(torch.cat([x, tmp], dim=1)[:, -oracle.cfg.n_ctx:], past=past_draft)
            next_d = logits_d[:, -1]
            probs = F.softmax(next_d / max(1e-5, temperature), dim=-1)
            tmp = torch.multinomial(probs, num_samples=1)
            draft_ids.append(tmp)
        proposed = torch.cat(draft_ids, dim=1)
        logits_o, past_oracle, _, _ = oracle(torch.cat([x, proposed], dim=1)[:, -oracle.cfg.n_ctx:], past=past_oracle)
        accepted = 0
        for step in range(proposed.size(1)):
            step_logits = logits_o[:, x.size(1) + step - 1]
            pred = step_logits.argmax(dim=-1, keepdim=True)
            if torch.all(pred == proposed[:, step:step+1]):
                accepted += 1
            else:
                break
        if accepted == 0:
            fallback_logits = logits_o[:, x.size(1) - 1]
            x = torch.cat([x, fallback_logits.argmax(dim=-1, keepdim=True)], dim=1)
        else:
            x = torch.cat([x, proposed[:, :accepted]], dim=1)
    return x[:, input_ids.size(1):]
